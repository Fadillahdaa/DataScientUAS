# -*- coding: utf-8 -*-
"""DataSaince UAS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Sycl7L2CHB1uScwMiIYaB8XYgxoUdaD
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv("dataset.csv", sep=";", header=1)

print("Shape:", df.shape)
display(df.head())

label_cols = df.columns[-7:].tolist()
feature_cols = df.columns[:-7].tolist()

X = df[feature_cols].copy()
y = df[label_cols].copy()

print("Fitur:", len(feature_cols), feature_cols)
print("Label:", len(label_cols), label_cols)

#@title Visualisasi Histogram 1
kandidat_num = ["SR","NR","OR","RR","BR"]
kol_num = [c for c in kandidat_num if c in df.columns]

corr = df[kol_num].corr()

plt.figure(figsize=(6,5))
plt.imshow(corr.values, aspect="auto")
plt.title("Heatmap Korelasi Fitur Numerik/Ordinal")
plt.xticks(range(len(kol_num)), kol_num, rotation=45, ha="right")
plt.yticks(range(len(kol_num)), kol_num)
plt.colorbar(label="Korelasi")
plt.tight_layout()
plt.show()

#@title Visualisasi Histogram 2
plt.figure(figsize=(6,4))
plt.scatter(df["SR"], df["NR"], alpha=0.7)
plt.title("Scatter SR vs NR")
plt.xlabel("SR (luas permukaan)")
plt.ylabel("NR (jumlah waduk)")
plt.tight_layout()
plt.show()

#@title Visualisasi Histogram 3
positif = y.sum().sort_values(ascending=False)

plt.figure(figsize=(9,4))
plt.bar(positif.index, positif.values)
plt.title("Jumlah data positif (1) per spesies")
plt.xlabel("Spesies")
plt.ylabel("Jumlah positif")
plt.xticks(rotation=30, ha="right")
plt.show()

persen = (y.mean()*100).sort_values(ascending=False)
display(pd.DataFrame({"positif_count": positif, "positif_%": persen.round(2)}))

#@title Data cleaning
missing_per_col = df.isna().sum()
missing_total = df.isna().sum().sum()
missing_pct = (missing_total / df.size) * 100
print("Missing total:", missing_total, f"({missing_pct:.2f}%)")

dup_total = df.duplicated().sum()
print("Duplicate rows:", dup_total)

label_cols = df.columns[-7:].tolist()
feature_cols = df.columns[:-7].tolist()

X = df[feature_cols].copy()
y = df[label_cols].copy().astype(int)

if "SR" in X.columns:
    X["SR_log"] = np.log1p(X["SR"])

X.head(), y.head()

#@title Feature Engineering
if "SR" in X.columns and "SR_log" not in X.columns:
    X["SR_log"] = np.log1p(X["SR"])

#@title Data Transformation
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

drop_cols = []
for c in ["ID", "Motorway", "Motorway (MV)"]:
    if c in X.columns:
        drop_cols.append(c)

X_model = X.drop(columns=drop_cols) if drop_cols else X.copy()
print("Drop kolom:", drop_cols)

num_cols = X_model.select_dtypes(include=["int64","float64","int32","float32"]).columns.tolist()
cat_cols = [c for c in X_model.columns if c not in num_cols]

print("Numerik:", num_cols)
print("Kategorikal:", cat_cols)

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
    ],
    remainder="drop"
)

#@title Data Splitting
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_model, y, test_size=0.2, random_state=42
)

print("Train:", X_train.shape, y_train.shape)
print("Test :", X_test.shape, y_test.shape)

#@title Model 1
import time
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, hamming_loss

X_train_t = preprocessor.fit_transform(X_train)
X_test_t = preprocessor.transform(X_test)

model_baseline = OneVsRestClassifier(
    LogisticRegression(
        C=1.0,
        solver="lbfgs",
        max_iter=2000,
        class_weight="balanced"
    )
)

t0 = time.time()
model_baseline.fit(X_train_t, y_train)
train_time_baseline = time.time() - t0

y_pred_baseline = model_baseline.predict(X_test_t)

print(f"Training time (detik): {round(train_time_baseline, 4)}")

micro_f1 = f1_score(y_test, y_pred_baseline, average="micro", zero_division=0)
macro_f1 = f1_score(y_test, y_pred_baseline, average="macro", zero_division=0)
h_loss   = hamming_loss(y_test, y_pred_baseline)

print("=== Hasil Baseline Logistic Regression (OvR) ===")
print("Micro-F1 :", round(micro_f1, 4))
print("Macro-F1 :", round(macro_f1, 4))
print("Hamming Loss:", round(h_loss, 4))

#@title Model 2
from sklearn.ensemble import RandomForestClassifier

model_advanced = MultiOutputClassifier(
    RandomForestClassifier(
        n_estimators=500,
        max_depth=None,
        random_state=42,
        n_jobs=-1,
        class_weight="balanced_subsample"
    ),
    n_jobs=-1
)

t0 = time.time()
model_advanced.fit(X_train_t, y_train)
train_time_advanced = time.time() - t0

y_pred_advanced = model_advanced.predict(X_test_t)

micro_f1 = f1_score(y_test, y_pred_advanced, average="micro", zero_division=0)
macro_f1 = f1_score(y_test, y_pred_advanced, average="macro", zero_division=0)
h_loss   = hamming_loss(y_test, y_pred_advanced)

print("=== Hasil Advanced Random Forest ===")
print("Training time (detik):", round(train_time_advanced, 4))
print("Micro-F1 :", round(micro_f1, 4))
print("Macro-F1 :", round(macro_f1, 4))
print("Hamming Loss:", round(h_loss, 4))

#@title Model 3
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
X_train_dl = X_train_t.toarray() if hasattr(X_train_t, "toarray") else np.array(X_train_t)
X_test_dl  = X_test_t.toarray()  if hasattr(X_test_t, "toarray")  else np.array(X_test_t)

y_train_dl = y_train.values.astype("float32")
y_test_dl  = y_test.values.astype("float32")

input_dim = X_train_dl.shape[1]
output_dim = y_train_dl.shape[1]

print("Input dim:", input_dim)
print("Output dim:", output_dim)

model_dl = keras.Sequential([
    layers.Input(shape=(input_dim,)),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(64, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(output_dim, activation="sigmoid")
])

model_dl.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss="binary_crossentropy",
    metrics=[keras.metrics.AUC(name="auc")]
)

model_dl.summary()

print("Total parameters:", model_dl.count_params())

early_stop = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=10,
    restore_best_weights=True
)

epochs = 30      # >= 10 (syarat)
batch_size = 32

t0 = time.time()
history = model_dl.fit(
    X_train_dl, y_train_dl,
    validation_split=0.2, # Use validation_split instead of validation_data
    epochs=epochs,
    batch_size=batch_size,
    callbacks=[early_stop],
    verbose=1
)
train_time_dl = time.time() - t0

print("Training time (detik):", round(train_time_dl, 4))

# Loss
plt.figure(figsize=(7,4))
plt.plot(history.history["loss"], label="train_loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.title("Training & Validation Loss per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# AUC
plt.figure(figsize=(7,4))
plt.plot(history.history["auc"], label="train_auc")
plt.plot(history.history["val_auc"], label="val_auc")
plt.title("Training & Validation AUC per Epoch")
plt.xlabel("Epoch")
plt.ylabel("AUC")
plt.legend()
plt.show()

from sklearn.metrics import f1_score, hamming_loss, roc_auc_score

y_proba_dl = model_dl.predict(X_test_dl)

threshold = 0.5
y_pred_dl = (y_proba_dl >= threshold).astype(int)

micro_f1 = f1_score(y_test_dl, y_pred_dl, average="micro", zero_division=0)
macro_f1 = f1_score(y_test_dl, y_pred_dl, average="macro", zero_division=0)
h_loss   = hamming_loss(y_test_dl, y_pred_dl)

try:
    auc_micro = roc_auc_score(y_test_dl, y_proba_dl, average="micro")
except:
    auc_micro = None

print("=== Hasil Deep Learning (MLP Keras) ===")
print("Training time (detik):", round(train_time_dl, 4))
print("Micro-F1 :", round(micro_f1, 4))
print("Macro-F1 :", round(macro_f1, 4))
print("Hamming Loss:", round(h_loss, 4))
print("ROC-AUC micro:", None if auc_micro is None else round(float(auc_micro), 4))

import pandas as pd
print("\nTRUE (5 data):")
display(pd.DataFrame(y_test_dl[:5], columns=label_cols))

print("PRED (5 data):")
display(pd.DataFrame(y_pred_dl[:5], columns=label_cols))

from sklearn.metrics import f1_score, hamming_loss

def evaluasi_multilabel(y_true, y_pred, nama="Model"):
    micro = f1_score(y_true, y_pred, average="micro", zero_division=0)
    macro = f1_score(y_true, y_pred, average="macro", zero_division=0)
    hloss = hamming_loss(y_true, y_pred)
    print(f"== {nama} ==")
    print("Micro-F1:", round(micro, 4))
    print("Macro-F1:", round(macro, 4))
    print("Hamming Loss:", round(hloss, 4))
    return micro, macro, hloss

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

def plot_cm_per_label(y_true, y_pred, label_cols, max_plot=7, judul_prefix="CM"):
    for i, lab in enumerate(label_cols[:max_plot]):
        cm = confusion_matrix(y_true.iloc[:, i], y_pred[:, i])
        plt.figure(figsize=(3.6,3))
        plt.imshow(cm, aspect="auto")
        plt.title(f"{judul_prefix} - {lab}")
        plt.xticks([0,1], ["0","1"])
        plt.yticks([0,1], ["0","1"])
        plt.xlabel("Pred")
        plt.ylabel("True")
        for r in range(2):
            for c in range(2):
                plt.text(c, r, cm[r, c], ha="center", va="center")
        plt.colorbar()
        plt.tight_layout()
        plt.show()

from sklearn.metrics import classification_report

def report_per_label(y_true, y_pred, label_cols):
    for i, lab in enumerate(label_cols):
        print("\nLabel:", lab)
        print(classification_report(y_true.iloc[:, i], y_pred[:, i], zero_division=0))

# baseline
report_per_label(y_test, y_pred_baseline, label_cols)

# random forest
report_per_label(y_test, y_pred_advanced, label_cols)

import numpy as np
import matplotlib.pyplot as plt

def rf_feature_importance_multioutput(model_multi, feature_names):
    # model_multi = MultiOutputClassifier(RandomForestClassifier)
    importances = []
    for est in model_multi.estimators_:
        importances.append(est.feature_importances_)
    mean_imp = np.mean(importances, axis=0)

    idx = np.argsort(mean_imp)[::-1]
    top = idx[:15]

    plt.figure(figsize=(8,4))
    plt.bar([feature_names[i] for i in top], mean_imp[top])
    plt.title("Top Feature Importance (Random Forest)")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()

# karena preprocessing kamu tidak one-hot, nama fiturnya adalah kolom X_model
feature_names = X_model.columns.tolist()
rf_feature_importance_multioutput(model_advanced, feature_names)

import pandas as pd
import matplotlib.pyplot as plt

hasil = [
    {"Model":"Baseline (LogReg OvR)", "Micro-F1":0.5339, "Macro-F1":0.4577, "Hamming Loss":0.3872, "Training Time (s)":0.0279, "ROC-AUC micro": None},
    {"Model":"Advanced (Random Forest)", "Micro-F1":0.7014, "Macro-F1":0.5110, "Hamming Loss":0.2368, "Training Time (s)":9.0999, "ROC-AUC micro": None},
    {"Model":"Deep Learning (MLP Keras)", "Micro-F1":0.6667, "Macro-F1":0.3941, "Hamming Loss":0.2594, "Training Time (s)":5.0654, "ROC-AUC micro": 0.8245},
]

df_hasil = pd.DataFrame(hasil)
display(df_hasil)

# bar chart micro-f1
plt.figure(figsize=(7,4))
plt.bar(df_hasil["Model"], df_hasil["Micro-F1"])
plt.title("Perbandingan Micro-F1")
plt.xticks(rotation=20, ha="right")
plt.tight_layout()
plt.show()

# bar chart macro-f1
plt.figure(figsize=(7,4))
plt.bar(df_hasil["Model"], df_hasil["Macro-F1"])
plt.title("Perbandingan Macro-F1")
plt.xticks(rotation=20, ha="right")
plt.tight_layout()
plt.show()

# bar chart hamming loss (lebih kecil lebih baik)
plt.figure(figsize=(7,4))
plt.bar(df_hasil["Model"], df_hasil["Hamming Loss"])
plt.title("Perbandingan Hamming Loss (lebih kecil lebih baik)")
plt.xticks(rotation=20, ha="right")
plt.tight_layout()
plt.show()

# (opsional) bar chart training time
plt.figure(figsize=(7,4))
plt.bar(df_hasil["Model"], df_hasil["Training Time (s)"])
plt.title("Perbandingan Training Time (detik)")
plt.xticks(rotation=20, ha="right")
plt.tight_layout()
plt.show()

import joblib
import os

# Create a directory to save models if it doesn't exist
output_dir = "trained_models"
os.makedirs(output_dir, exist_ok=True)

# Save Logistic Regression model
joblib.dump(model_baseline, os.path.join(output_dir, "logistic_regression_model.joblib"))
print(f"Logistic Regression model saved to {os.path.join(output_dir, 'logistic_regression_model.joblib')}")

# Save Random Forest model
joblib.dump(model_advanced, os.path.join(output_dir, "random_forest_model.joblib"))
print(f"Random Forest model saved to {os.path.join(output_dir, 'random_forest_model.joblib')}")

# Save Keras Deep Learning model
# Using the native Keras format (recommended)
model_dl.save(os.path.join(output_dir, "keras_dl_model.keras")) # Added .keras extension
print(f"Keras Deep Learning model saved to {os.path.join(output_dir, 'keras_dl_model.keras')}")

print("\nAll models have been saved. You can find them in the 'trained_models' directory.")